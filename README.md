# Data Engineering with Apache Spark, Delta Lake, and Lakehouse



**Create scalable pipelines that ingest, curate, and aggregate complex data in a timely and secure way**

## What is this book about?
In the world of ever-changing data and schemas, it is important to build data pipelines that can auto-adjust to changes. This book will help you build scalable data platforms that managers, data scientists, and data analysts can rely on.

This book covers the following exciting features: 
* Discover the challenges you may face in the data engineering world
* Add ACID transactions to Apache Spark using Delta Lake
* Understand effective design strategies to build enterprise-grade data lakes
* Explore architectural and design patterns for building efficient data ingestion pipelines
* Orchestrate a data pipeline for preprocessing data using Apache Spark and Delta Lake APIs



## Instructions and Navigations
All of the code is organized into folders. For example, Chapter02.

The code will look like the following:
```
const df = new DataFrame({...})
df.plot("my_div_id").<chart type>
```

**Following is what you need for this book:**
This book is for aspiring data engineers and data analysts who are new to the world of data engineering and are looking for a practical guide to building scalable data platforms. If you already work with PySpark and want to use Delta Lake for data engineering, you'll find this book useful. Basic knowledge of Python, Spark, and SQL is expected.

With the following software and hardware list you can run all code files present in the book (Chapter 1-12).

### Software and Hardware List

| Chapter  | Software required                   | OS required                        |
| -------- | ------------------------------------| -----------------------------------|
| 1 - 12         | Azure                     | Windows, Mac OS X, and Linux (Any) |




